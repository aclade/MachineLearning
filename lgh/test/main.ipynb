{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "import bs4\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_lists(soup):\n",
    "\n",
    "    lists = []\n",
    "    list_count_dict = {}\n",
    "\n",
    "    if soup.find('a', text='More lists with this book...'):\n",
    "\n",
    "        lists_url = soup.find('a', text='More lists with this book...')['href']\n",
    "\n",
    "        source = urlopen('https://www.goodreads.com' + lists_url)\n",
    "        soup = bs4.BeautifulSoup(source, 'lxml')\n",
    "        lists += [' '.join(node.text.strip().split()) for node in soup.find_all('div', {'class': 'cell'})]\n",
    "\n",
    "        i = 0\n",
    "        while soup.find('a', {'class': 'next_page'}) and i <= 10:\n",
    "\n",
    "            time.sleep(2)\n",
    "            next_url = 'https://www.goodreads.com' + soup.find('a', {'class': 'next_page'})['href']\n",
    "            source = urlopen(next_url)\n",
    "            soup = bs4.BeautifulSoup(source, 'lxml')\n",
    "\n",
    "            lists += [node.text for node in soup.find_all('div', {'class': 'cell'})]\n",
    "            i += 1\n",
    "\n",
    "        # Format lists text.\n",
    "        for _list in lists:\n",
    "            _list_name = _list.split()[:-2][0]\n",
    "            _list_count = int(_list.split()[-2].replace(',', ''))\n",
    "            list_count_dict[_list_name] = _list_count\n",
    "\n",
    "    return list_count_dict\n",
    "\n",
    "\n",
    "def get_shelves(soup):\n",
    "\n",
    "    shelf_count_dict = {}\n",
    "    \n",
    "    if soup.find('a', text='See top shelves‚Ä¶'):\n",
    "\n",
    "        # Find shelves text.\n",
    "        shelves_url = soup.find('a', text='See top shelves‚Ä¶')['href']\n",
    "        source = urlopen('https://www.goodreads.com' + shelves_url)\n",
    "        soup = bs4.BeautifulSoup(source, 'lxml')\n",
    "        shelves = [' '.join(node.text.strip().split()) for node in soup.find_all('div', {'class': 'shelfStat'})]\n",
    "        \n",
    "        # Format shelves text.\n",
    "        shelf_count_dict = {}\n",
    "        for _shelf in shelves:\n",
    "            _shelf_name = _shelf.split()[:-2][0]\n",
    "            _shelf_count = int(_shelf.split()[-2].replace(',', ''))\n",
    "            shelf_count_dict[_shelf_name] = _shelf_count\n",
    "\n",
    "    return shelf_count_dict\n",
    "\n",
    "\n",
    "def get_genres(soup):\n",
    "    genres = []\n",
    "    for node in soup.find_all('div', {'class': 'left'}):\n",
    "        current_genres = node.find_all('a', {'class': 'actionLinkLite bookPageGenreLink'})\n",
    "        current_genre = ' > '.join([g.text for g in current_genres])\n",
    "        if current_genre.strip():\n",
    "            genres.append(current_genre)\n",
    "    return genres\n",
    "\n",
    "\n",
    "def get_series_name(soup):\n",
    "    series = soup.find(id=\"bookSeries\").find(\"a\")\n",
    "    if series:\n",
    "        series_name = re.search(r'\\((.*?)\\)', series.text).group(1)\n",
    "        return series_name\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def get_series_uri(soup):\n",
    "    series = soup.find(id=\"bookSeries\").find(\"a\")\n",
    "    if series:\n",
    "        series_uri = series.get(\"href\")\n",
    "        return series_uri\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def get_top_5_other_editions(soup):\n",
    "    other_editions = []\n",
    "    for div in soup.findAll('div', {'class': 'otherEdition'}):\n",
    "      other_editions.append(div.find('a')['href'])\n",
    "    return other_editions\n",
    "\n",
    "def get_isbn(soup):\n",
    "    try:\n",
    "        isbn = re.findall(r'nisbn: [0-9]{10}' , str(soup))[0].split()[1]\n",
    "        return isbn\n",
    "    except:\n",
    "        return \"isbn not found\"\n",
    "\n",
    "def get_isbn13(soup):\n",
    "    try:\n",
    "        isbn13 = re.findall(r'nisbn13: [0-9]{13}' , str(soup))[0].split()[1]\n",
    "        return isbn13\n",
    "    except:\n",
    "        return \"isbn13 not found\"\n",
    "\n",
    "\n",
    "def get_rating_distribution(soup):\n",
    "    distribution = re.findall(r'renderRatingGraph\\([\\s]*\\[[0-9,\\s]+', str(soup))[0]\n",
    "    distribution = ' '.join(distribution.split())\n",
    "    distribution = [int(c.strip()) for c in distribution.split('[')[1].split(',')]\n",
    "    distribution_dict = {'5 Stars': distribution[0],\n",
    "                         '4 Stars': distribution[1],\n",
    "                         '3 Stars': distribution[2],\n",
    "                         '2 Stars': distribution[3],\n",
    "                         '1 Star':  distribution[4]}\n",
    "    return distribution_dict\n",
    "\n",
    "\n",
    "def get_num_pages(soup):\n",
    "    if soup.find('span', {'itemprop': 'numberOfPages'}):\n",
    "        num_pages = soup.find('span', {'itemprop': 'numberOfPages'}).text.strip()\n",
    "        return int(num_pages.split()[0])\n",
    "    return ''\n",
    "\n",
    "\n",
    "def get_year_first_published(soup):\n",
    "    year_first_published = soup.find('nobr', attrs={'class':'greyText'})\n",
    "    if year_first_published:\n",
    "        year_first_published = year_first_published.string\n",
    "        return re.search('([0-9]{3,4})', year_first_published).group(1)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def get_id(bookid):\n",
    "    pattern = re.compile(\"([^.-]+)\")\n",
    "    return pattern.search(bookid).group()\n",
    "    \n",
    "def scrape_book(book_id):\n",
    "    url = 'https://www.goodreads.com/book/show/' + book_id\n",
    "    source = urlopen(url)\n",
    "    soup = bs4.BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    return {'book_id_title':        book_id,\n",
    "            'book_id':              get_id(book_id),\n",
    "            'book_title':           ' '.join(soup.find('h1', {'id': 'bookTitle'}).text.split()),\n",
    "            \"book_series\":          get_series_name(soup),\n",
    "            \"book_series_uri\":      get_series_uri(soup),\n",
    "            'top_5_other_editions': get_top_5_other_editions(soup),\n",
    "            'isbn':                 get_isbn(soup),\n",
    "            'isbn13':               get_isbn13(soup),\n",
    "            'year_first_published': get_year_first_published(soup),\n",
    "            'authorlink':           soup.find('a', {'class': 'authorName'})['href'],\n",
    "            'author':               ' '.join(soup.find('span', {'itemprop': 'name'}).text.split()),\n",
    "            'num_pages':            get_num_pages(soup),\n",
    "            'genres':               get_genres(soup),\n",
    "            'shelves':              get_shelves(soup),\n",
    "            'lists':                get_all_lists(soup),\n",
    "            'num_ratings':          soup.find('meta', {'itemprop': 'ratingCount'})['content'].strip(),\n",
    "            'num_reviews':          soup.find('meta', {'itemprop': 'reviewCount'})['content'].strip(),\n",
    "            'average_rating':       soup.find('span', {'itemprop': 'ratingValue'}).text.strip(),\n",
    "            'rating_distribution':  get_rating_distribution(soup)}\n",
    "\n",
    "def condense_books(books_directory_path):\n",
    "\n",
    "    books = []\n",
    "    \n",
    "    # Look for all the files in the directory and if they contain \"book-metadata,\" then load them all and condense them into a single file\n",
    "    for file_name in os.listdir(books_directory_path):\n",
    "        if file_name.endswith('.json') and not file_name.startswith('.') and file_name != \"all_books.json\" and \"book-metadata\" in file_name:\n",
    "            _book = json.load(open(books_directory_path + '/' + file_name, 'r')) #, encoding='utf-8', errors='ignore'))\n",
    "            books.append(_book)\n",
    "\n",
    "    return books\n",
    "\n",
    "def main():\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    script_name = os.path.basename(__file__)\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--book_ids_path', type=str)\n",
    "    parser.add_argument('--output_directory_path', type=str)\n",
    "    parser.add_argument('--format', type=str, action=\"store\", default=\"json\",\n",
    "                        dest=\"format\", choices=[\"json\", \"csv\"],\n",
    "                        help=\"set file output format\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    book_ids              = [line.strip() for line in open(args.book_ids_path, 'r') if line.strip()]\n",
    "    books_already_scraped =  [file_name.replace('_book-metadata.json', '') for file_name in os.listdir(args.output_directory_path) if file_name.endswith('.json') and not file_name.startswith('all_books')]\n",
    "    books_to_scrape       = [book_id for book_id in book_ids if book_id not in books_already_scraped]\n",
    "    condensed_books_path   = args.output_directory_path + '/all_books'\n",
    "\n",
    "    for i, book_id in enumerate(books_to_scrape):\n",
    "        try:\n",
    "            print(str(datetime.now()) + ' ' + script_name + ': Scraping ' + book_id + '...')\n",
    "            print(str(datetime.now()) + ' ' + script_name + ': #' + str(i+1+len(books_already_scraped)) + ' out of ' + str(len(book_ids)) + ' books')\n",
    "\n",
    "            book = scrape_book(book_id)\n",
    "            # Add book metadata to file name to be more specific\n",
    "            json.dump(book, open(args.output_directory_path + '/' + book_id + '_book-metadata.json', 'w'))\n",
    "\n",
    "            print('=============================')\n",
    "\n",
    "        except HTTPError as e:\n",
    "            print(e)\n",
    "            exit(0)\n",
    "\n",
    "\n",
    "    books = condense_books(args.output_directory_path)\n",
    "    if args.format == 'json':\n",
    "        json.dump(books, open(f\"{condensed_books_path}.json\", 'w'))\n",
    "    elif args.format == 'csv':\n",
    "        json.dump(books, open(f\"{condensed_books_path}.json\", 'w'))\n",
    "        book_df = pd.read_json(f\"{condensed_books_path}.json\")\n",
    "        book_df.to_csv(f\"{condensed_books_path}.csv\", index=False, encoding='utf-8')\n",
    "        \n",
    "    print(str(datetime.now()) + ' ' + script_name + f':\\n\\nüéâ Success! All book metadata scraped. üéâ\\n\\nMetadata files have been output to /{args.output_directory_path}\\nGoodreads scraping run time = ‚è∞ ' + str(datetime.now() - start_time) + ' ‚è∞')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "839e0350e26a03edc91ebdedcb53fe8f541802bd102f926973002a967871fd0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
