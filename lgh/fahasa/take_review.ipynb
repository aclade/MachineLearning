{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FAHASA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Khởi Tạo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import urllib.request as urllib2\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import concurrent.futures\n",
    "from multiprocessing import Pool\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import *\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from selenium.webdriver.common.by import By\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import argparse\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import os\n",
    "import regex as re\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementNotInteractableException, ElementClickInterceptedException, ElementNotVisibleException, StaleElementReferenceException\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import geckodriver_autoinstaller\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []\n",
    "goc='https://www.fahasa.com/sach-trong-nuoc/van-hoc-trong-nuoc/tieu-thuyet.html' \n",
    "pages.append(goc)\n",
    "def get_links(trang):\n",
    "    for i in range(2,trang+1):\n",
    "        pages.append('https://www.fahasa.com/sach-trong-nuoc/van-hoc-trong-nuoc/tieu-thuyet.html?order=num_orders&limit=24&p='+str(i))\n",
    "get_links(86)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon = []\n",
    "img = []\n",
    "loun = []\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.119 Safari/537.36\"}\n",
    "\n",
    "def take_name_and_img(url):\n",
    "    r = requests.get(url,headers=headers)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    s = soup.find_all('span', class_ ='product-image')\n",
    "    for ptd in s:\n",
    "        tam = ptd.find('img')\n",
    "        lon.append(tam['alt'])\n",
    "        img.append(tam['data-src'])\n",
    "    s = soup.find_all('a', class_='product-image')\n",
    "    for ptd in s:\n",
    "        loun.append(ptd['href'])\n",
    "    \n",
    "for url in pages:\n",
    "    take_name_and_img(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lop = []\n",
    "loa = []\n",
    "genre = []\n",
    "\n",
    "def take_prices_inf_genre(url):\n",
    "    r = requests.get(url,headers=headers)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    s = soup.find('span',class_='price')\n",
    "    try:\n",
    "        lop.append(s.text)\n",
    "        s = soup.find('div',class_='std')\n",
    "        loa.append(s.text)\n",
    "        s = soup.find('ol',class_='breadcrumb').find_all('a')\n",
    "        temp = []\n",
    "        for ptd in s:\n",
    "            temp.append(ptd.text)\n",
    "        genre.append(temp)\n",
    "    except:\n",
    "        lon.pop(len(lop))\n",
    "        img.pop(len(lop))\n",
    "    \n",
    "\n",
    "for url in loun:\n",
    "    take_prices_inf_genre(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pro={\"Ten \": lon,\"Gia \": lop ,\"Thong tin\": loa,\"Anh\":img,\"The loai\":genre}\n",
    "df=pd.DataFrame(dict_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"lgh.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lonn = []\n",
    "def no_accent_vietnamese(s):\n",
    "    s = re.sub('[áàảãạăắằẳẵặâấầẩẫậ]', 'a', s)\n",
    "    s = re.sub('[ÁÀẢÃẠĂẮẰẲẴẶÂẤẦẨẪẬ]', 'A', s)\n",
    "    s = re.sub('[éèẻẽẹêếềểễệ]', 'e', s)\n",
    "    s = re.sub('[ÉÈẺẼẸÊẾỀỂỄỆ]', 'E', s)\n",
    "    s = re.sub('[óòỏõọôốồổỗộơớờởỡợ]', 'o', s)\n",
    "    s = re.sub('[ÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢ]', 'O', s)\n",
    "    s = re.sub('[íìỉĩị]', 'i', s)\n",
    "    s = re.sub('[ÍÌỈĨỊ]', 'I', s)\n",
    "    s = re.sub('[úùủũụưứừửữự]', 'u', s)\n",
    "    s = re.sub('[ÚÙỦŨỤƯỨỪỬỮỰ]', 'U', s)\n",
    "    s = re.sub('[ýỳỷỹỵ]', 'y', s)\n",
    "    s = re.sub('[ÝỲỶỸỴ]', 'Y', s)\n",
    "    s = re.sub('đ', 'd', s)\n",
    "    s = re.sub('Đ', 'D', s)\n",
    "    return s\n",
    "\n",
    "def tach(vcl):\n",
    "    for i in range(len(vcl)):\n",
    "        if vcl[i] == '(' or vcl[i]=='-':\n",
    "            return vcl[:i]\n",
    "    return vcl\n",
    "goc0 = 'https://www.goodreads.com/'\n",
    "goc1 = 'https://www.goodreads.com/search?q='\n",
    "newlist = []\n",
    "\n",
    "for ptd in lon:\n",
    "    lonn.append(tach(ptd))\n",
    "for lgh in lonn:\n",
    "    url = goc1\n",
    "    for ptd in lgh:\n",
    "        if ptd == ' ':\n",
    "            url += '%20'\n",
    "        else:\n",
    "            url+= ptd\n",
    "\n",
    "    url = no_accent_vietnamese(url)\n",
    "    try:\n",
    "        html=urlopen(url) \n",
    "        bs = BeautifulSoup(html.read(), 'html.parser') \n",
    "        ptd = bs.find_all('a',class_='bookTitle')\n",
    "        newlist.append(goc0+ptd[0]['href'])\n",
    "    except:\n",
    "        newlist.append('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf = []\n",
    "rating = []\n",
    "all_genres = []\n",
    "def genre_about(url):\n",
    "    try: \n",
    "        html=urlopen(url) \n",
    "        bs = BeautifulSoup(html.read(), 'html.parser') \n",
    "        dtb = re.findall(r'renderRatingGraph\\([\\s]*\\[[0-9,\\s]+', str(bs))[0]\n",
    "        dtb = ' '.join(dtb.split())\n",
    "        dtb = [int(c.strip()) for c in dtb.split('[')[1].split(',')]\n",
    "        dtb_dict = {'5': dtb[0],'4': dtb[1],'3': dtb[2],'2': dtb[3],'1':  dtb[4]}\n",
    "        genres = []\n",
    "        for node in bs.find_all('div', {'class': 'left'}):\n",
    "            current_genres = node.find_all('a', {'class': 'actionLinkLite bookPageGenreLink'})\n",
    "            current_genre = ' > '.join([g.text for g in current_genres])\n",
    "            if current_genre.strip():\n",
    "                genres.append(current_genre)\n",
    "        ptd = bs.find('div',id=\"description\")\n",
    "        inf.append(ptd.text)\n",
    "        rating.append(dtb_dict)\n",
    "        all_genres.append(genres)\n",
    "    except:\n",
    "        inf.append('')\n",
    "        rating.append('')\n",
    "        all_genres.append('')\n",
    "\n",
    "for ptd in newlist:\n",
    "    genre_about(ptd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = []\n",
    "user_rating = []\n",
    "user_rating_book = []\n",
    "options = webdriver.FirefoxOptions()\n",
    "options.headless = True \n",
    "def get_rating1(driver,book):\n",
    "    soup = driver.page_source\n",
    "    source =BeautifulSoup(soup, 'html.parser')\n",
    "    for ptd in source.find_all('a',class_='left imgcol'):\n",
    "        user_rating_book.append(book)\n",
    "        user_id.append(ptd['href'])\n",
    "    for ptd in source.find_all('div',class_='reviewHeader uitext stacked'):\n",
    "        if ptd.find_all('span',class_='staticStars notranslate')== []:\n",
    "            user_rating.append('0')\n",
    "        else:\n",
    "            user_rating.append(ptd.find('span',class_='staticStars notranslate').text)\n",
    "    return \n",
    "\n",
    "def main(url,book): \n",
    "    try:\n",
    "        driver = webdriver.Firefox(executable_path='firefoxdriver', options=options)\n",
    "        driver.get(url)\n",
    "        kt = True\n",
    "        select = Select(driver.find_element(By.NAME, str('language_code')))\n",
    "        select.select_by_value('en')\n",
    "        time.sleep(4)\n",
    "        old = len(user_id)\n",
    "        get_rating1(driver,book)\n",
    "        if (old == len(user_id)) :\n",
    "            kt = False\n",
    "            time.sleep(10)\n",
    "        page_counter = 1\n",
    "        while (True):\n",
    "            try:\n",
    "                if driver.find_element(By.XPATH, \"//a[@class='next_page']\"):\n",
    "                    driver.find_element(By.XPATH, \"//a[@class='next_page']\").click()\n",
    "                    time.sleep(3)\n",
    "                    get_rating1(driver,book)\n",
    "            except :\n",
    "                os.system(\"taskkill /im firefox.exe /f\")\n",
    "                return \n",
    "    except:\n",
    "        user_rating_book.append('')\n",
    "        user_id.append('')\n",
    "        user_rating.append('')\n",
    "        os.system(\"taskkill /im firefox.exe /f\")\n",
    "                \n",
    "for i in range(len(newlist)):\n",
    "    main(newlist[i],lon[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_pro={\"id \": user_id,\"rating \": user_rating ,\"name\": user_rating_book}\n",
    "df=pd.DataFrame(dict_pro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"lgh6.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "839e0350e26a03edc91ebdedcb53fe8f541802bd102f926973002a967871fd0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
